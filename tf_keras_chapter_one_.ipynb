{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.keras_chapter_one.",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPM9Go0C7m16"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "train_samples = []"
      ],
      "metadata": {
        "id": "0cLJXlPI7sVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "    # The ~5% of younger individuals who did experience side effects\n",
        "    random_younger = randint(13,64)\n",
        "    train_samples.append(random_younger)\n",
        "    train_labels.append(1)\n",
        "\n",
        "    # The ~5% of older individuals who did not experience side effects\n",
        "    random_older = randint(65,100)\n",
        "    train_samples.append(random_older)\n",
        "    train_labels.append(0)\n",
        "\n",
        "for i in range(1000):\n",
        "    # The ~95% of younger individuals who did not experience side effects\n",
        "    random_younger = randint(13,64)\n",
        "    train_samples.append(random_younger)\n",
        "    train_labels.append(0)\n",
        "\n",
        "    # The ~95% of older individuals who did experience side effects\n",
        "    random_older = randint(65,100)\n",
        "    train_samples.append(random_older)\n",
        "    train_labels.append(1)\n",
        "    "
      ],
      "metadata": {
        "id": "lqTgk3eN71pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_samples:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "ogLp41Hd7859"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_labels:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "0Ad9Qwm08Bm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "train_samples = np.array(train_samples)\n",
        "train_labels, train_samples = shuffle(train_labels, train_samples)"
      ],
      "metadata": {
        "id": "5UdJr-1H8Fnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "3XNW02J28JJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in scaled_train_samples:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "U58vsZjC8M1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(scaled_train_samples)"
      ],
      "metadata": {
        "id": "7m_L72ql8QRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_samples.shape"
      ],
      "metadata": {
        "id": "pSJS1YIj8Tt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "52xOaPuB8XQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create An Artificial Neural Network With TensorFlow's Keras API"
      ],
      "metadata": {
        "id": "HX5p8U3D8pKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "KTmrH09d8r00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "kqsZMk3O8xK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(units=32, input_shape=(1,), activation='relu'),\n",
        "    Dense(units=64, activation='relu'),\n",
        "#     Dense(units=2, activation='sigmoid')\n",
        "    Dense(units=4, activation='softmax')\n",
        "])   "
      ],
      "metadata": {
        "id": "eyIbOI_h80z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model is an instance of a Sequential object. A tf.keras.Sequential model is a linear stack of layers. It accepts a list, and each element in the list should be a layer.\n",
        "\n",
        "As you can see, we have passed a list of layers to the Sequential constructor. Let's go through each of the layers in this list now. Note, if you don’t explicitly set an activation function, then Keras will use the linear activation function.\n",
        "\n",
        "First Hidden Layer Our first layer is a Dense layer. This type of layer is our standard fully-connected or densely-connected neural network layer. The first required parameter that the Dense layer expects is the number of neurons or units the layer has, and we’re arbitrarily setting this to 32.\n",
        "\n",
        "Additionally, the model needs to know the shape of the input data. For this reason, we specify the shape of the input data in the first hidden layer in the model (and only this layer). The parameter called input_shape is how we specify this.\n",
        "\n",
        "As discussed, we’ll be training our network on the data that we generated and processed in the previous episode, and recall, this data is one-dimensional. The input_shape parameter expects a tuple of integers that matches the shape of the input data, so we correspondingly specify (1,) as the input_shape of our one-dimensional data.\n",
        "\n",
        "You can think of the way we specify the input_shape here as acting as an implicit input layer. The input layer of a neural network is the underlying raw data itself, therefore we don't create an explicit input layer. This first Dense layer that we're working with now is actually the first hidden layer.\n",
        "\n",
        "Lastly, an optional parameter that we’ll set for the Dense layer is the activation function to use after this layer. We’ll use the popular choice of relu.\n",
        "\n",
        "2nd hidden layer also the same as the above menioned Output Layer Lastly, we specify the output layer. This layer is also a Dense layer, and it will have 2 neurons. This is because we have two possible outputs: either a patient experienced side effects, or the patient did not experience side effects.\n",
        "\n",
        "This time, the activation function we’ll use is softmax, which will give us a probability distribution among the possible outputs."
      ],
      "metadata": {
        "id": "Gp1YtFs09HL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "MXDG3O0P9Iuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4RsgfyN39Oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function configures the model for training and expects a number of parameters. First, we specify the optimizer Adam. Adam accepts an optional parameter learning_rate, which we’ll set to 0.0001.\n",
        "\n",
        "The next parameter we specify is loss. We’ll be using sparse_categorical_crossentropy, given that our labels are in integer format.\n",
        "\n",
        "Note that when we have only two classes, we could instead configure our output layer to have only one output, rather than two, and use binary_crossentropy as our loss, rather than categorical_crossentropy. Both options work equally well and achieve the exact same result.\n",
        "\n",
        "With binary_crossentropy, however, the last layer would need to use sigmoid, rather than softmax, as its activation function.\n",
        "\n",
        "Moving on, the last parameter we specify in compile() is metrics. This parameter expects a list of metrics that we’d like to be evaluated by the model during training and testing. We’ll set this to a list that contains the string ‘accuracy’."
      ],
      "metadata": {
        "id": "OYFGRzxY9Z9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10,validation_split=0.1, epochs=40, shuffle=True, verbose=2)"
      ],
      "metadata": {
        "id": "r3PZ-trA9cFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0qbCR84P9Z2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we specify verbose=2. This just specifies how much output to the console we want to see during each epoch of training. The verbosity levels range from 0 to 2, so we’re getting the most verbose output.\n",
        "\n",
        "When we call fit() on the model, the model trains, and we get this output.\n",
        "\n",
        "What Is A Validation Set?\n",
        "\n",
        "Recall that we previously built a training set on which we trained our model. With each epoch that our model is trained, the model will continue to learn the features and characteristics of the data in this training set.\n",
        "\n",
        "The hope is that later we can take this model, apply it to new data, and have the model accurately predict on data that it hasn’t seen before based solely on what it learned from the training set.\n",
        "\n",
        "Now, let’s discuss where the addition of a validation set comes into play.\n",
        "\n",
        "Before training begins, we can choose to remove a portion of the training set and place it in a validation set. Then, during training, the model will train only on the training set, and it will validate by evaluating the data in the validation set.\n",
        "\n",
        "Essentially, the model is learning the features of the data in the training set, taking what it's learned from this data, and then predicting on the validation set. During each epoch, we will see not only the loss and accuracy results for the training set, but also for the validation set.\n",
        "\n",
        "This allows us to see how well the model is generalizing on data it wasn’t trained on because, recall, the validation data should not be part of the training data.\n",
        "\n",
        "This also helps us see whether or not the model is overfitting. Overfitting occurs when the model only learns the specifics of the training data and is unable to generalize well on data that it wasn’t trained on."
      ],
      "metadata": {
        "id": "cBM1N6Vs9x7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network Predictions With TensorFlow's Keras API\n",
        "\n",
        "\n",
        "Neural Network Predictions With TensorFlow's Keras API¶\n",
        "We’ll create a test set in the same fashion for which we created the training set. In general, the test set should always be processed in the same way as the training set.\n",
        "\n",
        "We won’t go step-by-step over the code that generates and processes the test data below, as it has already been covered in detail where we generated the training data,"
      ],
      "metadata": {
        "id": "S6iSMVmV93jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels =  []\n",
        "test_samples = []"
      ],
      "metadata": {
        "id": "1BkC28Cr-A4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    # The 5% of younger individuals who did experience side effects\n",
        "    random_younger = randint(13,64)\n",
        "    test_samples.append(random_younger)\n",
        "    test_labels.append(1)\n",
        "    \n",
        "    # The 5% of older individuals who did not experience side effects\n",
        "    random_older = randint(65,100)\n",
        "    test_samples.append(random_older)\n",
        "    test_labels.append(0)\n",
        "\n",
        "for i in range(200):\n",
        "    # The 95% of younger individuals who did not experience side effects\n",
        "    random_younger = randint(13,64)\n",
        "    test_samples.append(random_younger)\n",
        "    test_labels.append(0)\n",
        "    \n",
        "    # The 95% of older individuals who did experience side effects\n",
        "    random_older = randint(65,100)\n",
        "    test_samples.append(random_older)\n",
        "    test_labels.append(1)"
      ],
      "metadata": {
        "id": "S6RZcuXg-F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labales =np.array(test_labels)\n",
        "test_samples =np.array(test_samples)\n",
        "test_labales,test_samples=shuffle(test_labales, test_samples)"
      ],
      "metadata": {
        "id": "Mq5NRJT--K1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "1HfCtqYn-OPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions"
      ],
      "metadata": {
        "id": "-6iTUrGW-WRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x=scaled_test_samples, batch_size=10,verbose=0)"
      ],
      "metadata": {
        "id": "ZynBAuMg-XUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To this function, we pass in the test samples x, specify a batch_size, and specify which level of verbosity we want from log messages during prediction generation. The output from the predictions won't be relevant for us, so we're setting verbose=0 for no output.\n",
        "\n",
        "Note that, unlike with training and validation sets, we do not pass the labels of the test set to the model during the inference stage.\n",
        "\n",
        "To see what the model's predictions look like, we can iterate over them and print them out."
      ],
      "metadata": {
        "id": "Zj5is9iR-ehc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(predictions)"
      ],
      "metadata": {
        "id": "vWO_OlJt-fud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in predictions:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "psTXUT7j-kI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rounded_predictions = np.argmax(predictions, axis=-1)"
      ],
      "metadata": {
        "id": "Yr5DHd0X-nbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in rounded_predictions:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "VWho_RYY-qH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix"
      ],
      "metadata": {
        "id": "Wx3EeNzA-w2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j_gU-tzC-ynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm =confusion_matrix(y_true=test_labales, y_pred=rounded_predictions)"
      ],
      "metadata": {
        "id": "GWnNYgNF-2Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm"
      ],
      "metadata": {
        "id": "MBJPy0vT-5UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    "
      ],
      "metadata": {
        "id": "zHP0ci48-7T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_plot_labels=['no_side_effects','had_side_effects']\n",
        "plot_confusion_matrix(cm=cm,classes=cm_plot_labels,title='Confusion_matrix');"
      ],
      "metadata": {
        "id": "Pb3wsjZr_APs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the plot of the confusion matrix, we have the predicted labels on the x-axis and the true labels on the y-axis. The blue cells running from the top left to bottom right contain the number of samples that the model accurately predicted. The white cells contain the number of samples that were incorrectly predicted.\n",
        "\n",
        "There are 420 total samples in the test set. Looking at the confusion matrix, we can see that the model accurately predicted 393 out of 420 total samples. The model incorrectly predicted 25 out of the 420.\n",
        "\n",
        "For the samples the model got correct, we can see that it accurately predicted that the patients would experience no side effects 195 times. It incorrectly predicted that the patient would have no side effects 10 times when the patient did actually experience side effects.\n",
        "\n",
        "On the other side, the model accurately predicted that the patient would experience side effects 200 times that the patient did indeed experience side effects. It incorrectly predicted that the patient would have side effects 15 times when the patient actually did not experience side effects.\n",
        "\n",
        "As you can see, this is a good way we can visually interpret how well the model is doing at its predictions and understand where it may need some work.\n",
        "\n",
        "Saving And Loading The Model In Its Entirety\n",
        "If we want to save a model at its current state after it was trained so that we could make use of it later, we can call the save() function on the model. To save(), we pass in the file path and name of the file we want to save the model to with an h5 extension."
      ],
      "metadata": {
        "id": "FP7rbllt_NBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "if os.path.isfile('models/medical_trial_model.h5') is False:\n",
        "    model.save('models/medical_trial_model.h5')"
      ],
      "metadata": {
        "id": "Kdno2C3v_URE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('models/medical_trial_model.h5')"
      ],
      "metadata": {
        "id": "tW0ySCxNAanV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.summary()"
      ],
      "metadata": {
        "id": "Kv2kOyReAj60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.get_weights()"
      ],
      "metadata": {
        "id": "h5AXXHmMAn42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.optimizer"
      ],
      "metadata": {
        "id": "2jZgTDmhAqzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2model.to_json()"
      ],
      "metadata": {
        "id": "lgm42SF5A1Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_string = model.to_json()\n",
        "json_string"
      ],
      "metadata": {
        "id": "dDmdEJsTA37M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "model_architecture=model_from_json(json_string)"
      ],
      "metadata": {
        "id": "Hdgwqm5CA80k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By printing the summary of the model, we can verify that the new model has the same architecture of the model that was previously saved."
      ],
      "metadata": {
        "id": "TO1-TGurBDOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_architecture.summary()"
      ],
      "metadata": {
        "id": "_SEfsaQ-BEPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, we can also use this same approach to saving and loading the model architecture to and from a YAML string. To do so, we use the functions to_yaml() and model_from_yaml() in the same fashion as we called the json functions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "-Saving And Loading The Weights Of The Model The last saving mechanism we’ll discuss only saves the weights of the model.\n",
        "We can do this by calling model.save_weights() and passing in the path and file name to save the weights to with an h5 extension."
      ],
      "metadata": {
        "id": "17vl4EN_BNN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('my_model_weights.h5')"
      ],
      "metadata": {
        "id": "ULyvvWKDBTOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential([\n",
        "    Dense(units=32, input_shape=(1,), activation='relu'),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dense(units=4, activation='softmax')\n",
        "])\n",
        "model2.load_weights('my_model_weights.h5')"
      ],
      "metadata": {
        "id": "rmAAq9yjBXrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ve now seen how to save only the weights of a model and deploy those weights to a new model, how to save only the architecture and then deploy that architecture to a model, and how to save everything about a model and deploy it in its entirety at a later time. Each of these saving and loading mechanisms may come in useful in differing scenarios.*italicized text:\\*"
      ],
      "metadata": {
        "id": "fan3HUuOBdQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Qk6OM3UBiqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}